# -*- coding: utf-8 -*-
"""DQN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FHpcrvkKE64wsHXATJYV0lqGiq1oS2kG
"""

import numpy as np
import gym
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation, Flatten, Conv2D, MaxPooling2D
from tensorflow.keras.optimizers import Adam
from skimage.color import rgb2gray
from skimage.transform import resize
from tensorflow.keras.optimizers import RMSprop
from tensorflow.keras import backend as K
import random


gamma = 0.99  # Discount factor for past rewards
epsilon = 1.0  # Epsilon greedy parameter
epsilon_min = 0.1  # Minimum epsilon greedy parameter
epsilon_max = 1.0  # Maximum epsilon greedy parameter
epsilon_interval = (
    epsilon_max - epsilon_min
)  # Rate at which to reduce chance of random action being taken
batch_size = 32  # Size of batch taken from replay buffer
max_steps_per_episode = 3000
num_actions = 4
ATARI_SHAPE = (84, 84, 4)
ACTION_SIZE = 4


#pre processing function to reduce the size of the input and to convert rgb input to grey scale
def pre_processing(observe):
    processed_observe = np.uint8(
        resize(rgb2gray(observe), (84 , 84), mode='constant') * 255)
    return processed_observe


#Building the convolution network 
def atari_model():    
    # Input layers.
    frames_input = keras.layers.Input(ATARI_SHAPE, name='frames')
    actions_input = keras.layers.Input((ACTION_SIZE,), name='mask')

    # Assuming that the input frames are still encoded from 0 to 255. Transforming to [0, 1].
    normalized = keras.layers.Lambda(lambda x: x / 255.0)(frames_input)
    
    # "The first hidden layer convolves 32 8×8 filters with stride 4 with the input image and applies a rectifier nonlinearity."
    conv_1 = keras.layers.Conv2D(
        32, (8, 8), strides=4, activation='relu', name="Conv1"
    )(normalized)
    
    # "The second hidden layer convolves 64 4×4 filters with stride 2, followed by a rectifier nonlinearity."
    conv_2 = keras.layers.Conv2D(
        64, (4, 4), strides=2, activation='relu', name="Conv2"
    )(conv_1)
    
    # "The third hidden layer convolves 64 3x3 filters with stride 1, again followed by a rectifier nonlinearity."
    conv_3 = keras.layers.Conv2D(
        64, (3, 3), strides=1, activation='relu', name="Conv3"
    )(conv_2)
    
    # Flattening the convolutional layer.
    conv_flattened = keras.layers.Flatten(name="flatten")(conv_3)
    
    # "Fully connected layer made up of 512 rectifier units."
    hidden = keras.layers.Dense(512, activation='relu', name="Dense512")(conv_flattened)
    
    # The output layer is a fully-connected linear layer with a single output for each valid action."
    output = keras.layers.Dense(ACTION_SIZE, name="Output")(hidden)
    
    # Finally, we multiply the output by the mask.
    filtered_output = keras.layers.Multiply(name='QValue')([output, actions_input])

    model = keras.models.Model(inputs=[frames_input, actions_input], outputs=filtered_output)
    optimizer = keras.optimizers.RMSprop(lr=0.00025, rho=0.95, epsilon=0.01)
    model.compile(optimizer, loss=huber_loss)
    model.summary()
    return model
model = atari_model()
model_target = atari_model()


replay_memory = []  #[state,action,reward,next_state,done_history]
episode_reward_history = []
mean_reward = 0
episode_count = 0
frame_count = 0
epsilon_random = 50000
epsilon_greedy = 100000
max_memory = 100000
update_after_action = 4
update_target_network = 10000
loss_fun = keras.losses.Huber()
env = gym.make("BreakoutNoFrameskip-v4")
ACTION_SIZE = 4
num_episode = 100000


while True:
    loss = 0.0
    state = np.array(env.reset())
    state = pre_processing(state)
    history = np.stack((state, state, state, state), axis=2)
    history = np.reshape([history], (1, 84, 84, 4))
    episode_reward = 0

    #Choosing the action based on the epsilon value so that the exploration exploitation dillema is taken care 
    for i in range(1,max_steps_per_episode):
        frame_count += 1
        if epsilon > np.random.uniform(0,1) or frame_count < epsilon_random:
            action = np.random.choice(num_actions)
        else:
            q_value = model.predict([history, np.ones(ACTION_SIZE).reshape(1, ACTION_SIZE)]) #Using the CNN model to predict q value 
            action = np.argmax(q_value[0])
        
        #Reducing the epsilon value with time
        epsilon = epsilon_interval/epsilon_greedy            
        epsilon = max(epsilon,epsilon_min)

        #taking action that was chosen by the CNN network
        next_state,reward,done,info = env.step(action)
        next_state = pre_processing(next_state)
        next_state = np.reshape([next_state], (1, 84, 84, 1))
        next_history = np.append(next_state, history[:, :, :, :3], axis=3)
        episode_reward += reward

        #Storing the values so that they can be used to train the network later
        replay_memory.append((history,action,reward,next_history,done))
        state = next_state
        
        #Randomly selecting values from memory to train the network
        if frame_count % update_after_action == 0 and len(replay_memory) > batch_size:
            mini_batch = random.sample(replay_memory, batch_size)
            
            state_samples = np.zeros((batch_size, ATARI_SHAPE[0],
                                ATARI_SHAPE[1], ATARI_SHAPE[2]))
            next_state_samples = np.zeros((batch_size, ATARI_SHAPE[0],
                                    ATARI_SHAPE[1], ATARI_SHAPE[2]))
            updated_q = np.zeros((batch_size,))
            action_samples, reward_samples, done_samples = [], [], []            
            
            for idx, val in enumerate(mini_batch):
                state_samples[idx] = val[0]
                next_state_samples[idx] = val[3]
                action_samples.append(val[1])
                reward_samples.append(val[2])
                done_samples.append(val[4])
            
            actions_mask = np.ones((batch_size, ACTION_SIZE))
            next_Q_values = model_target.predict([next_state_samples, actions_mask])

            for i in range(batch_size):
                if done_samples[i]:
                    updated_q[i] = -1
                else:
                    updated_q[i] = reward_samples[i] + gamma * np.amax(next_Q_values[i])                      
            
            #Training the model
            mask = tf.one_hot(action_samples,num_actions)
            h = model.fit([state_samples, mask], updated_q, epochs=1,
            batch_size=batch_size, verbose=0)
            loss += h.history['loss'][0]
                      
        if frame_count % update_target_network == 0:
            model_target.set_weights(model.get_weights())
            print ("Target Model Refreshed")
            template = "episode reward: {:.2f} at episode {}, frame count {}"
            print(template.format(episode_reward, episode_count, frame_count))

        if len(replay_memory) > max_memory:
            del replay_memory[0]                            
        if done:
            break

    episode_reward_history.append(episode_reward)
    if len(episode_reward_history) > 100:
        del episode_reward_history[0]
    mean_reward = np.mean(episode_reward_history)    

    episode_count +=1

    if mean_reward > 40:
        print("Solved at episode {}!".format(episode_count))
        break

